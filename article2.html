<!DOCTYPE html>
<html lang="en">

<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Using Autoencoders to Generate Skeleton-based Typograph</title>
</head>

<body>

    <aside>
        <p>sideNav</p>
    </aside>
    <main>
        <article>
            <header>
                <h1>Thirteen Ways of Looking At a Typeface</h1>
                <span class="author">Jéssica Parente, Luís Gonçalo
                </span>
            </header>
            <section class="article2-intro">
                <h2>1. Introduction</h2>
                <p>
                    The design of type has undergone numerous changes over time [4]. In the early years, typography was
                    seen as a system made up of a series of rules. The artistic movements that arrived at the beginning
                    of the twentieth century rejected the historical forms and transformed outdated aspects of visual
                    language and expression. However, projects that combined software, arts and design only appeared a
                    few years later with the proliferation of personal computers, allowing programming to reach a wider
                    audience. Thanks to all these changes, the tools to design type changed, and new possibilities for
                    typographic experimentation appeared, resulting in (i) grammar-based techniques that explore the
                    principle of database amplification (e.g. [2]); (ii) evolutionary systems that breed design
                    solutions under the direction of a designer (e.g. [22, 15]); (iii) or even, Machine Learning (ML)
                    systems that learn the glyphs features to build new ones (e.g. [14]) [18]. These computational
                    approaches can also be helpful as a starting point of inspiration.
                </p>
                <img src="/"
                    alt="Fig. 1. Interpolation of the skeleton and stroke width from two existing A’s (light blue and red) resulting in a new A (dark blue)">
                <p>
                    Most emerging fonts continue to be developed by type designers who study the shape of each letter
                    and its design with great precision, despite the emergence of these new possibilities. Type design
                    is a hugely complex discipline, and its expertise ensures typography quality [28]. Moreover, with
                    the proliferation of web typography and online reading, the use of variable and dynamic fonts has
                    increased, allowing more options for font designers and font users. Additionally, visual identities
                    created nowadays are becoming more dynamic [17]. Museums, institutions, organisations, events and
                    media increasingly rely on this type of identity. Consequently, designers should adapt their work to
                    these new possibilities by creating dynamic identities with animations and mutations. Even though
                    new computer systems create expressive and out-of-the-box results, they do not have the knowledge of
                    an expert. But this is also an advantage, allowing non-arbitrary exploitation that extends the range
                    of possibilities. It is necessary to create a balance to take advantage of the computational systems
                    and the expert labour. Moreover, most generative systems that design type focus on the letters’
                    filling and don’t see the structure of a glyph as a variation parameter.
                </p>
                <p>
                    To overcome these limitations, we propose an Autoregressive model [9] that creates new glyph
                    skeletons by the interpolation of existing ones. Our skeleton-based approach uses glyphs skeletons
                    of existing fonts as input to ensure the quality of the generated results. The division of the
                    structure and the filling of the glyphs add variability to the results. Different glyphs can be
                    created by just changing the structure or the filling. </p>
                <p>
                    The proposed approach enables the exploration of a continuous range of font styles by navigating on
                    the Autoencoder (AE) learnt latent space. With the results of this approach, it is also possible to
                    apply different filling methods that use the stroke width of the original letters to produce new
                    glyphs. </p>
            </section>
            <section class="article2-2">
                <h2>2. Related Work</h2>
                <p>
                    Over time, the methods and technologies available for type design have improved and designers have
                    to evolve and adapt their process of thinking in accordance. Generative Adversarial Networks (GANs)
                    have revealed impressive advances, presenting high-resolution images nearly indistinguishable from
                    the real ones. In the typographic field, they are helpful when one wishes to obtain coherent glyphs
                    in a typeface. When designing a typeface, one has to simultaneously seek an aesthetically appealing
                    result and coherence among the different glyphs. This can be facilitated by exploring the
                    similarities between the same letter present across diverse fonts, and the transferred stylistic
                    elements within the same font [5]. Balashova et al. [2] develop a stroke-based geometric model for
                    glyphs, a fitting procedure to re-parametrise arbitrary fonts to capture these correlations. The
                    framework uses a manifold learning technique that allows for interactively improving the fit quality
                    and interpolating, adding or removing stylistic elements in existing fonts. Campbell and Kautz [3]
                    develop a similar contour-based framework allowing the editing of a glyph and the propagation of
                    stylistic elements across the entire alphabet. Phan et al. [19] and Suveeranont and Igarashi [26]
                    present two different frameworks that give one or more outline-based glyphs of several characters as
                    input, producing a complete typeface that bears a similar style to the inputs. Rehling and
                    Hofstadter [21] use one or more grid-based lowercase letters to generate the rest of the Roman
                    alphabet, creating glyphs that share different style features. Azadi et al. [1] develop an
                    end-to-end stacked conditional GAN model to generate a set of highly-stylised glyph images following
                    a consistent style from very few examples.
                </p>
                <img src="/" alt="Black and White Interpolation of the skeleton and stroke width from two existing A’s">
                <p>
                    We can also imitate the behaviour of a variable font using Recurrent Neural Networks (RNNs) and
                    interpolate to obtain intermediate results. Lopes et al. [14] model the drawing process of fonts by
                    building sequential generative models of vector graphics. Their model provides a scale-invariant
                    representation of imagery. The latent representation may be systematically exploited to achieve
                    style propagation. Shamir and Rappoport [24] present a parametric feature-based font design
                    approach. The development of a visual design system and the use of constraints for preserving the
                    designer’s intentions create a more natural environment in which high-level parametric behaviours
                    can be defined. By changing the glyph parameters they create several family instances. Also, outside
                    the typographic field, there are some good examples exploring the latent space. Sketch-RNN [7] is an
                    RNN able to construct stroke-based drawings. The network produces sketches of common objects in a
                    vector format and explores the latent space interpolation of various vector images. There is also
                    increased attention to these networks and their application to facilitate the use and combination of
                    fonts. A usual way to combine different fonts is by using fonts from the same family or created by
                    the same designer. Another way is to find fonts that match x-height and ascenders/descenders.
                    Fontjoy [20] is another tool to facilitate the process of mixing and matching typefaces and choosing
                    fonts to use side by side. FontMap [8] and Font-VAE [10] are tools developed with the goal of
                    discovering alternative fonts with the same aesthetics.
                </p>
            </section>
            <section class="article2-3">
                <h2>3. Approach</h2>
                <p>In this section, we present the developed model that generates new letter skeletons by interpolating
                    existing ones. This process allows us to control the style of the resulting font by navigating the
                    latent space. We explain all the steps taken, from the data collection and editing, passing through
                    the development of the network architecture until the experimentation and analysis of the results.
                </p>
                <img src="/" alt="Fig. 2. Diagram of the architecture of our approach.">
                <h3>
                    3.1. Data
                </h3>
                <p>
                    One of the most important aspects of our approach is the collection and pre-processing of the
                    dataset. We compile a collection of fonts in TTF font format with different weights from Google
                    Fonts [6]. This dataset is composed of five different font styles, Serif, Sans Serif, Display,
                    Handwriting and Monospace. We opted not to use handwriting and display fonts because they were
                    largely distinct from the rest, which is not desirable for our approach. Their ornamental component,
                    sometimes not even filled, complicates the extraction of a representative skeleton. We only worked
                    with 26 characters (A-Z) of the Latin alphabet in their capital format. We believed that, as a work
                    in progress, it would be best to create a dataset with a few characters. By just using capital
                    letters, we are reducing the complexity of the approach. </p>
                <p>
                    After selecting the fonts, we remained with 2623 TTF files. Then, we use the library Skelefont [16]
                    to extract the skeleton of a font file. It applies the Zhang-Suen Thinning Algorithm [29] to derive
                    the structural lines of a binary image. This library also allows the extraction of the points of the
                    skeletons as well as the connections between them. It can also calculate the distance between the
                    points and their closest borderline pixel, returning the stroke width of the original glyph at each
                    of these points. </p>
                <p>
                    For each font, we rasterise the vectors that compose the skeleton of each glyph into a 64x64px black
                    and white image. We also save all points’ positions and stroke width of the original glyph in a file
                    to use later to generate the filling of the glyphs. Then, we repeat the process for the 26 letters
                    of the alphabet (capital letters of the Latin alphabet only). This process is shown in the first
                    three images of the diagram.
                </p>
                <img src="/"
                    alt="Fig. 3. Comparison between the originals (left) and the reconstructed skeletons (right).">
                <h3>
                    3.2. Network Architecture
                </h3>
                <p>The proposed model consists of a Conditional Variational Autoencoder (VAE) [11] and an Autoregressive
                    sketch decoder. We used a VAE instead of a regular AE to allow us to manipulate the latent vectors
                    more easily. The output of the VAE are the parameters of distribution instead of vectors in the
                    latent space. Moreover, the VAE imposes a constraint on this latent distribution forcing it to be a
                    normal distribution which makes sure that the latent space is regularised. Therefore, we can create
                    smoother transitions between different fonts when we sample the latent space moving from one cluster
                    to the other. The Conditional part of the model allows us to input which letter we are encoding and
                    decoding allowing us to manipulate better which letter we are creating. Finally, as all the letters
                    share the same latent space we can also explore the skeletons between different letters. </p>
                <p>
                    Figure 2 shows a diagram of the architecture used. In summary, the encoder employs a Convolutional
                    Neural Network (CNN) that processes the greyscale images and encodes them into two 64-D latent
                    vectors which consist of a set of means (μ) and standard deviations (σ) of a Gaussian
                    representation. Through experimentation, we found that size 64 for the latent code presents the best
                    results for our approach as it is a good trade-off, allowing us to compress all the characteristics
                    of the letter while keeping its tractability. Then, using the mean and standard deviation we take a
                    sample from the Gaussian representation z to be used as input for both decoders, the image decoder
                    and the sketch decoder. The image decoder consists of a set of convolutional transpose layers that
                    receive the z vector and decodes it into a greyscale image which is compared with the original
                    input. The sketch decoder consists of an LSTM [9] with dropout [25, 23] that transforms the z vector
                    into a sequence of 30 points creating a single continuous path. This path is rasterised using a
                    differentiable vector graphics library [13] to produce an output image. This library allows
                    converting vector data to a raster representation while facilitating backpropagation between the two
                    domains. In the rasterisation process, we take the sequence of 30 x and y values and transform them
                    to canvas coordinates. Then, we create a line that connects all points following the same order they
                    are returned from the sketch decoder. The width of this path needs to be carefully selected to match
                    the width of the original skeleton. If the width of the path is thinner than in the original images,
                    at some part of the training process, the network stops trying to compose the whole letter and
                    starts to fill the width of the letter in a zig-zag manner. However, if the line is thicker than in
                    the original images we lose detail in the final skeleton. </p>
                <p>
                    Finally, we render the produced path in a canvas as a greyscale image that is compared with the
                    original image. Although the standard VAE works at the pixel level, the output of our sketch decoder
                    is a sequence of points, thus allow- ing the generation of scalable vector graphics that allow
                    easier manipulation of the generated skeletons without losing quality. The loss value is calculated
                    in a similar way as in the standard VAEs. We calculate the Binary Cross Entropy between the output
                    images of the image decoder and the original inputs. We also calculate the Kullback-Leibler
                    Divergence [12] to allow a regularised distribution of the latent space. Finally, we compute the
                    Binary Cross Entropy between the original inputs and the output of the sketch decoder. To obtain the
                    final loss value we add the three values together.</p>
            </section>
            <section class="article2-4">
                <h2>
                    4. RESULTS
                </h2>
                <p>
                    The VAE and sketch decoder trained for 50 epochs with a learning rate of 0.001 and a batch size of
                    256. As mentioned before, we use 2623 64 × 64px black and white images of skeletons for each capital
                    letter of the Latin alphabet, so our dataset is constituted of 68 198 images.
                </p>
                <h3>
                    4.1. Reconstruction of Skeletons
                </h3>
                <p>
                    As mentioned before, the model returns a sequence of points that, when connected, create a
                    reconstruction of the skeleton image used as input. In most cases, the generated strokes reconstruct
                    the basic features of the skeleton. For example, in the case of the letter “A”, the network first
                    creates one stem, then the crossbar connects both stems and finally draws the second stem. Even
                    though there is nothing to control the distance between points or to enforce them to be close, the
                    network learns that it needs to connect both stems at the beginning and the end of the sequence.
                    Another interesting feature observable in the reconstruction is related to how the ANN handles the
                    letter “T”. This letter presents one of the simplest skeletons of the alphabet, so the network can
                    learn how to generate the whole structure of the letter very quickly in comparison with others. </p>
                <p>
                    Figure 3 presents a comparison between the original inputs and the reconstructed skeletons using a
                    single stroke. The reconstructions of “C”, “L” or “K”, for example, are very similar. The letters
                    “A”, “X” and “K” present a more complex challenge to the network as it needs to create a path that
                    overlaps itself to draw the whole letter structure with only one line. Sometimes, the serif is lost
                    in the reconstruction due to the same issue. The line must overlap itself multiple times to create
                    the small parts without messing with the overall structure of the letter. But the other reason for
                    this could be that the number of letters with serif is lower than the number of letters without it.
                    In summary, even though the small details of the letters might be lost, our network is able to
                    create the minimal structure of the letter, generating skeletons that cannot be confused with any
                    other letter.

                </p>
                <img src="/"
                    alt="Fig. 4. t-SNE visualisation of the learned latent space z for all the capital letters of the Latin alphabet.">

                <h3>
                    4.2. Latent Representation of Font Style
                </h3>
                <p>To understand if the trained model can learn a latent representation for the different letters that
                    is smooth and interpretable, we need to visualise the 64-dimensional z vectors for the dataset. So
                    we take all the images of the dataset (68198 images) and encode them using our network. Then, using
                    the means and standard deviations of each encoded image we took a sample from the distribution.
                    Finally, we took all the z vectors and reduced their dimensionality using the t-SNE algorithm [27].
                    This allows us to reduce the z vectors from a size of 64 to two dimensions which can be translated
                    to positions in a two-dimensional domain. For each position of a two-dimensional grid, we place the
                    image of the best candidate. We select this candidate by finding the two-dimensional encoding
                    closest to that position. Figure 4 presents the visualisation of the results. In general, the model
                    can separate the different letters into clusters. In some cases, it is also possible to observe that
                    similar letters are placed near each other, for example in the case of the letters “B”, “R” and “P”.
                    These three letters present similar anatomical characteristics, they share a top bowl and they all
                    have a vertical stem, thus they are placed near each other. The same happens for the letters “T” and
                    “I” which are placed more separately from the rest but near each other. Even though the majority of
                    the skeletons for the letter “I” is represented with a single stem, in some cases, when they have
                    serif, they are similar to the letter “T” but with a cross stroke on the top and bottom part of the
                    letter. This leads to both letters having a strong similarity between each other, therefore they are
                    placed together in the latent space.
                </p>
                <p>
                    We also create a similar representation contemplating the skeleton images of a single letter (2623
                    images). To understand if the trained model was able to smoothly change styles within the same
                    letter we created a similar visualisation as in Figure 4. Figure 5 presents the visualisation of the
                    results for the letter “R”. As it is possible to observe, the model is able to separate the
                    different font weights across the latent space, creating different regions. The zoom-in boxes show
                    four separate locations where we notice a concentration of specific font styles. In (A) it is
                    presented a region where the condensed fonts are, while the opposite corner (D) represents the most
                    extended fonts. It is also possible to observe that (B) represents the italic, and finally (C)
                    presents most of the fonts with serifs. Local changes within these regions are also visible, where
                    the font width increases when distancing from the region (A) and approximating to the region (D). It
                    is also possible to observe a slight increase in the font height in the top-bottom direction.</p>

            </section>

        </article>
    </main>
</body>

</html>